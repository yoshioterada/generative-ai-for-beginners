# 生成 AI アプリケーションのセキュリティ

[![Securing Your Generative AI Applications](../../images/13-lesson-banner.jpg?WT.mc_id=academic-105485-yoterada)]()

## はじめに  

このレッスンでは以下の内容を取り扱います：  

- AI システムにおけるセキュリティ  
- AI システムに対する一般的なリスクと脅威  
- AI システムを保護するための方法と考慮事項  

## 学習目標

このレッスンを修了すると、下記の内容を理解できます：

- AI システムに対する脅威とリスク
- AI システムを保護するための一般的な方法と実践
- セキュリティに関するテストの実施が予期せぬ結果やユーザーの信頼の低下を防ぐ方法

## 生成 AI におけるセキュリティとは何か

人工知能（AI）や機械学習（ML）技術が私たちの生活にますます影響を与える中で、顧客データだけでなく AI システム自体の保護も重要です。AI/ML の誤った判断が重大な結果をもたらす可能性がある一方で、さまざまな業界で重要な意思決定プロセスをサポートするためにますます使用されています。

下記は考慮すべき重要なポイントです：

- **AI/ML の影響**：AI/ML は日常生活に大きな影響を与えており、それらの保護が不可欠になっています。
- **セキュリティの課題**：AI/ML の影響は、悪意のあるユーザーや組織的なグループによる巧妙な攻撃から AI ベースの製品を守るため、適切な注意が必要です。。
- **戦略的な問題**：テクノロジー業界は、長期的な顧客の安全とデータセキュリティを保護するために、戦略的な課題に対して積極的に取り組む必要があります。

さらに、機械学習モデルは悪意のある入力と無害の異常データを区別するのが困難です。トレーニング・データの多くは、第三者が自由に追加できる、整理されていない公開データセットから得られます。攻撃者はデータセットに自由に追加できるため、データセットを侵害する必要はありません。時間が経つと、信頼性の低い悪意のあるデータも、データの構造やフォーマットが正しければ、信頼性の高いデータとして扱われるようになります。

だからこそ、モデルが意思決定に使用するデータの信頼性と保護は重要なのです。

## AI の脅威とリスクの理解

AI や関連システムに関して、データ・ポイズニングが現在最も重要なセキュリティの脅威として認識されています。データ・ポイズニングと、誰かが AI のトレーニングに使用するデータを意図的に変更し、それによって AI が誤った判断を下すようにする手法です。これは、標準化された検出方法や対策がないことに加えて、信頼できない、または整理されていない公開データセットを利用してトレーニングを行っているためです。データの信頼性を維持し、誤ったトレーニング・プロセスを防ぐためには、データの出所と経緯を追跡するのが重要です。さもなければ、「ゴミが入ればゴミが出る」という古い格言が当てはまり、モデルの信頼性が損なわれます。

下記は、データ・ポイズニングがモデルにどのように影響するかの例を示します：

1. **ラベルの反転**：バイナリ分類タスクで、敵対者がトレーニングデータの小さなサブセットのラベルを意図的に反転させます。例えば、無害なサンプルが悪意のあるものとしてラベル付けされ、モデルが誤った関連付けを学習するようになります。
 **例**：スパムフィルターが意図的に操作されたラベルのために、正当なメールをスパムと誤分類する。
2. **特徴のポイズニング**：攻撃者がトレーニングデータの特徴を微妙に変更してバイアスを導入したり、モデルを誤解させたりします。
 **例**：推薦システムを操作するために製品説明に無関係なキーワードを追加する。
3. **データの注入**：モデルの動作に影響を与えるためにトレーニングセットに悪意のあるデータを注入する。
 **例**：感情分析の結果を歪めるために偽のユーザーレビューを挿入する。
4. **バックドア攻撃**：敵対者がトレーニングデータに隠れたパターン（バックドア）を挿入します。モデルはこのパターンを認識するように学習し、トリガーされると悪意のある動作をします。
 **例**：特定の人物を誤認識するバックドア付きの画像でトレーニングされた顔認識システム。

MITRE Corporation は、悪意のあるユーザが AI システムに対する実際の攻撃で使用する戦術や技術に関する知識ベースである [ATLAS（人工知能システムの敵対的脅威の全体像）](https://atlas.mitre.org/?WT.mc_id=academic-105485-yoterada) を作成しました。
> AI 対応システムの脆弱性は増加しており、AI の導入により従来のサイバー攻撃を超えて既存システムの攻撃対象が拡大しています。AI がさまざまなシステムにますます組み込まれる中、これらの独自に進化する脆弱性に対する認識を高めるために ATLAS を開発しました。ATLAS は [MITRE ATT&CK](https://attack.mitre.org/?WT.mc_id=academic-105485-yoterada) フレームワークをモデルにしており、その TTP (戦術、技術、手順) は ATT&CK のものを補完します。

MITRE ATT&CK® フレームワークが、従来のサイバーセキュリティで高度な脅威シミュレーションシナリオの計画に広く使われているのと同様に、ATLAS は新たな攻撃に対する防御をよりよく理解し準備に役立つ、簡単に検索できる TTP（戦術、技術、手順）を提供します。

さらに、Open Web Application Security Project（OWASP）は、LLM を利用するアプリケーションで見つかった最も重大な脆弱性の「[トップ 10 リスト](https://llmtop10.com/?WT.mc_id=academic-105485-yoterada)」を作成しました。このリストは、前述のデータポイズニングなどの脅威のリスクと共に、下記のような他の脅威を強調しています：

- **プロンプト・インジェクション**：攻撃者は、アプリ実装者が慎重に作成した入力を上書きして大規模言語モデル（LLM）を操作し、意図と異なる動作を行う手法。
- **サプライチェーンの脆弱性**：LLM（大規模言語モデル）を使用するアプリケーションを構成する部品やソフトウェア、例えば Python のモジュールや外部のデータセットなどが、問題を抱えている場合があります。これにより、予期しない結果や偏見が生じたり、基盤となるインフラに脆弱性が生じたりすることがあります。
- **過度の依存**：LLM（大規模言語モデル）は間違いを犯すことがあり、不正確な結果や安全でない結果を出す場合があります。人々がその結果をそのまま信じ、予期せぬ現実世界での悪影響をもたらしたという事例もあります。

Microsoft のクラウド・アドボケイト Rod Trent は、こうした AI の脅威について深く掘り下げ、これらのシナリオに最適に対処するための幅広いガイダンスを提供する無料の電子書籍『[Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-yoterada)』を執筆しました。

## AI システムと LLM のセキュリティ・テスト

人工知能（AI）はさまざまな分野や産業を変革し、社会に新たな可能性や利益をもたらしています。しかし、AI に はデータのプライバシーや、バイアス（偏見）、説明責任の欠如、悪用の可能性など、重要な課題やリスクもあります。そのため、AI システムが倫理的および法的な基準を守り、ユーザーや関係者から信頼されるように、AI システムが安全かつ責任あるように実装しているのかを検証するのが重要です。

セキュリティテストとは、AI システムや LLM の脆弱性を特定し、そのセキュリティを評価するプロセスです。テストの目的や範囲に応じて、開発者、ユーザー、または第三者の監査人が実施することができます。AI システムや LLM のための一般的なセキュリティ・テスト方法は下記のとおりです：

- **データのサニタイズ**： これは、AI システムや LLM のトレーニング・データや入力から、機密情報や個人情報を削除したり匿名化したりするプロセスです。データのサニタイズは、機密情報や個人情報の露出を減らすことで、データの漏洩や悪意のある操作を防ぐのに役立ちます。
- **敵対的テスト**： これは、AI システムや LLM の入出力に対して、敵対的な例を生成し敵対的な攻撃に対する堅牢性と耐性を評価するプロセスです。敵対的なテストは、攻撃者に悪用される可能性のある AI システムや LLM の脆弱性や弱点を特定し、軽減するのに役立ちます。
- **モデル検証**： これは、AI システムや LLM のモデルパラメータや、アーキテクチャの正確さと完全性を確認するプロセスです。モデルの検証は、モデルが保護され認証済みかどうかを確認し、モデル盗難の検出や防止に役立ちます。

- **出力検証**： これは、AI システムや LLM の出力の品質と信頼性を確認するプロセスです。出力の検証は、出力が一貫して正確かどうかを確認し、悪意のある操作の検出し、その修正に役立ちます。

AI システムのリーダーである OpenAI は、AI に対する安全性の貢献を目的として、レッド・チーミング・ネットワーク・イニシアチブの一環として、一連の _安全性評価_ を設定しました。

> 評価は、簡単な Q&A テストからより複雑なシミュレーションまでさまざまです。具体例として、OpenAI がさまざまな角度から AI の行動を評価するために開発したサンプル評価を以下に示します：

#### 評価技術

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-yoterada): AI システムは、どれほど巧みに他の AI システムを騙して、秘密の言葉を言わせることができるでしょうか？
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-yoterada): AI システムは、どれほど巧みに他の AI システムを納得させて、お金を寄付させることができるでしょうか？
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-yoterada): AI システムは、どれほど巧みに他の AI システムに影響を与えて政治的な提案を支持させることができるでしょうか？

#### Steganography (情報を隠す技術)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-yoterada): AI システムは、どれほど上手に他の AI システムに見つからずに秘密メッセージを渡すことができるでしょうか？
- [テキスト圧縮](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-yoterada): AI システムは、秘密メッセージを隠すために、どれほど巧みにメッセージを圧縮・解凍できるでしょうか？
- [シェリング・ポイント](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-yoterada): AI システムは、直接のコミュニケーションなしで、どれくらい上手に他の AI システムと連携できるでしょうか？

### AI セキュリティ

AI システムを悪意のある攻撃や誤った使い方、予想外の結果から守るのが大切です。そのためには、AIシステムの安全性、信頼性、信頼性を確保するための対策が必要です。例えば：

- AI モデルをトレーニングし実行するために使用するデータとアルゴリズムを保護
- AI システムに対する不正アクセス、改ざん、破壊からの防御
- AI システムに含まれるバイアス(偏見)や差別、倫理的な問題を検出し軽減
- AI の決定や行動の責任、透明性、説明のしやすさを保証
- Aligning the goals and values of AI systems with those of humans and society
- AI システムの目標や価値観を人間社会のものと一致

AI のセキュリティは、AI システムとデータの信頼性、可用性、機密性を守るために重要です。AIのセキュリティに関する課題と機会には下記のようなものがあります。

- 機会: サイバーセキュリティ戦略にAIを取り入れると、脅威の特定や対応時間の短縮に重要な役割を果たす。AIはフィッシング、マルウェア、ランサムウェアなどのサイバー攻撃の検出と対策を自動化し、強化するために役立つ。
- 課題: AI は敵対者も利用し、偽の情報を生成したり、ユーザーになりすましたり、AIシステムの弱点を突いたりする高度な攻撃を仕掛ける。だからこそ、AI 開発者は、悪用に対して強く耐えるシステムを設計する特別な責任がある。

### データ保護

LLM は使用するデータのプライバシーとセキュリティにリスクをもたらす可能性があります。例えば、LLM はトレーニングデータから個人名、住所、パスワード、クレジットカード番号などの機密情報を記憶し、漏洩する可能性があります。また、悪意のある人々が LLM の脆弱性やバイアス（偏見）を利用しようとして、操作や攻撃を行う場合もあります。したがって、これらのリスクを認識し、LLM で使用するデータを保護するための適切な対策が重要です。LLM で使用するデータを保護するために取れる対策はいくつかります。

- **LLM に共有するデータ量と種類を制限**: 関連性のある必要データだけを共有し、機密性の高い、もしくは個人情報を含むデータは共有しないようにする。ユーザーは、LLM に共有するデータを特定できないようにしたり暗号化するのも重要で、例えば、識別情報を削除、もしくは隠蔽し、セキュアな通信チャネルを使用する
- **LLM が生成するデータを検証**: LLM が生成する出力結果が不要な情報や不適切な情報を含まないように、常に正確さと品質を確認する
- **データ漏洩や問題を報告・警告**: LLM が不適切なテキストを生成したり、異常な動作をする場合、データ漏洩やセキュリティ・インシデントの可能性があるので注意する

マルチクラウド環境でデータとAIの力を活用したい組織にとって、データのセキュリティ、ガバナンス、コンプライアンスは非常に重要です。すべてのデータを保護し管理するのは複雑で多くの作業が必要です。複数のクラウド上で、異なる種類のデータ（構造化データ、非構造化データ、AI が生成したデータ）を保護し管理する必要があり、既存および将来のデータセキュリティ、ガバナンス、AI 規制にも対応する必要があります。データを保護するために、いくつかのベストプラクティスと予防策を採用する必要があります。

- データ保護とプライバシー機能を提供するクラウドサービスやプラットフォームを利用
- データの誤りや不整合、異常を検知するために、データ品質と検証ツールを利用
- データが適切(responsible)で透明性(transparent)のある方法で使われるように、データガバナンスと倫理フレームワークを利用

### 現実の脅威を再現 - AI レッド・チーミング

現実の脅威を再現するのは、システムのリスクを特定し、防御策をテストするために同様のツール、戦術、手順を使用して、強固な AI システムを構築するための一般的な方法とされています。

> AI レッド・チーミングの実践は進化し、より広い意味を持つようになりました。それはセキュリティの脆弱性を見つけるだけでなく、有害なコンテンツの生成など、他のシステム障害も検知できるようになります。AI システムには新たなリスクがあり、レッド・チーミングは、プロンプト・インジェクションや根拠のないコンテンツの生成など、こうした新しいリスクを理解するための重要な方法です。 - [マイクロソフト AI レッド・チームがより安全なAIの未来を築く](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-yoterada)

[![Guidance and resources for red teaming](../../images/13-AI-red-team.png?WT.mc_id=academic-105485-yoterada)]()

下記は、マイクロソフトの AI レッド・チーム・プログラムに影響を与えた重要なポイントです。

1. **AI レッド・チーミングの幅広いスコープ:**
   AI レッド・チーミングは現在、セキュリティと責任あるAI（RAI）の両方の成果を含んでいます。従来、レッド・チーミングはセキュリティの側面に焦点を当て、モデルをベクトルとして扱っていました（例：ファウンデーションモデルの AI モデルの内部構造やアルゴリズム、データなどを取得）。しかし、AIシステムは新しいセキュリティの脆弱性をもたらし（例：プロンプトインジェクション、ポイズニング）、特別な注意が必要です。セキュリティだけでなく、AI レッド・チーミングは公平性の問題（例：ステレオタイプ化）や有害なコンテンツ（例：暴力発言の正当化）も調査します。これらの問題を早期に特定することで、防御に対する優先順位を付けられます。
1. **悪意ある失敗と無害な失敗:**
   AI レッドチーミングでは、悪意のある視点と無害な視点の両方から失敗を考慮します。例えば、新しい Bing をレッド・チーミングする際には、悪意のある攻撃者がシステムをどのように悪用するかだけでなく、一般ユーザーが問題のあるコンテンツや有害なコンテンツに遭遇する可能性も調べます。従来のセキュリティ・レッド・チーミングが主に悪意のある攻撃者に焦点を当てていたのに対し、AI レッドチーミングはより幅広い人を対象に潜在的な失敗を考慮します。
1. **AI システムの柔軟な性質:**
   AI アプリケーションは常に進化しています。大規模言語モデルのアプリケーションでは、開発者は変化する要件に対応しなければなりません。継続的なレッド・チーミングは、変化するニーズやリスクに対して警戒し適応が可能になります。

AI レッド・チーミングは万能ではなく、[ロールベース・アクセス制御（RBAC）](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-yoterada)や包括的なデータ管理ソリューションなど、他の追加の制御手段を補完するものと考えてください。これは、プライバシーとセキュリティを考慮しつつ、バイアス（偏見）や有害なコンテンツ、誤情報を最小限に抑えてユーザーの信頼を損なわないようにする、安全で責任ある AI ソリューションを採用するセキュリティ戦略を補完するためのものです。

こちらは、レッド・チーミングが AI システムのリスクを特定し軽減するのに、どのように役立つかをより詳しく理解するための追加の参考情報のリストです。

- [大規模言語モデル（LLM）とアプリケーションのレッド・チーミング計画](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-yoterada)
- [OpenAI レッド・チーミング・ネットワークについて](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-yoterada)
- [AI レッド・チーミング - 安全で責任ある AI ソリューションを構築するための重要な方法](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-yoterada)
- MITRE [ATLAS（AI システムに対する敵対的脅威の全体像）](https://atlas.mitre.org/?WT.mc_id=academic-105485-yoterada)は、悪意のあるユーザが AI システムに対して実際の攻撃で使用する戦術や技術をまとめた内容です。

## 知識チェック

データの整合性を維持し、不正使用を防ぐための良いアプローチは何ですか？

1. データ・アクセスとデータ管理には、強力なロール・ベースの制御を導入
1. データの誤表現や不正使用を防ぐため、データラベリングを導入し監査
1. AI インフラがコンテンツ・フィルタリングに対応しているか確認

正解：１
3つの提案はどれも素晴らしいですが、ユーザーに適切なデータアクセス権限を与えると、LLM が使うデータの操作や誤表現を防ぐのに非常に役立ちます。

## 🚀 チャレンジ

AI 時代における、[機密情報の管理と保護する方法](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-yoterada)についてもっと知識を深めてください。

## お疲れ様でした! 学習を続ける

このレッスン修了後、[生成 AI 学習コレクション](https://aka.ms/genai-collection?WT.mc_id=academic-105485-yoterada) をチェックして、Generative AI の知識をレベルアップさせましょう。

次のレッスン 14 では、[生成 AI アプリケーションのライフサイクル](../../../14-the-generative-ai-application-lifecycle/translations/ja-jp/README.md?WT.mc_id=academic-105485-yoterada)について学びます！